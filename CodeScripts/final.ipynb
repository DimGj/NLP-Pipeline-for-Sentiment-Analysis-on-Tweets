{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Load\n",
    "from email import header\n",
    "from fileinput import filename\n",
    "from genericpath import exists\n",
    "from operator import pos, truediv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "from tkinter import W\n",
    "from turtle import color, title\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import csv\n",
    "import sklearn.model_selection\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import gensim\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib.pyplot import clf\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some helpful functions which are repeated throughout the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads a Pickle file only\n",
    "def LoadFile(Filename):\n",
    "    Data = {} #empty dictionary to hold the dataframe\n",
    "    if exists(Filename): #check if the filepath exists\n",
    "        Data = pd.read_pickle(Filename)\n",
    "        return(Data)\n",
    "    else:\n",
    "        print(\"Directory not found!\") #error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves a Pickle file only\n",
    "def SaveFile(DataFrame,Filename): #save a dataframe to pickle format\n",
    "    if not exists(Filename): #if the file doesn't already exist\n",
    "        DataFrame.to_pickle(Filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opens a tsv file only\n",
    "def OpenFile(Filename): \n",
    "    if exists(Filename): #if the file path exists\n",
    "        Data = pd.read_csv(Filename,sep='\\t',lineterminator='\\n') #we call read_csv but with separator parameter tab so in reality we read a tsv\n",
    "        return Data\n",
    "    else:\n",
    "        print(\"Requested File does not exist!\") #error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocpt(pct,allvalues):\n",
    "    absolute = int(pct / 100.*sum(allvalues))\n",
    "    return \"{:.1f}%\\n({:d} g)\".format(pct, absolute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits a tuple\n",
    "def SplitTuple(TupleArray):\n",
    "    TupleStr = []\n",
    "    TupleValues = []\n",
    "    for items in TupleArray:\n",
    "        for moreitems in items:\n",
    "            if isinstance(moreitems,str): #if the items are of type string\n",
    "                TupleStr.append(moreitems) #append them to the Str list\n",
    "            else: #else they are of type float\n",
    "                TupleValues.append(moreitems) #append them to the values list\n",
    "    return TupleStr,TupleValues #return the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a Pie for the Data Analysis part\n",
    "def CreatePlot(ExplodeData,Colors,Values,StrArray,LegendPreview,LegendTitle):\n",
    "    Fig,Ax = plt.subplots(figsize=(12,9)) #set the size of the pie\n",
    "    WedgeProperties = { 'linewidth' : 1, 'edgecolor' : \"green\" }\n",
    "    wedges,texts,autotexts = Ax.pie(Values,autopct=lambda pct: autocpt(pct, Values),\n",
    "                                    explode=ExplodeData,labels=StrArray,shadow=True,\n",
    "                                    colors=Colors,startangle=90,wedgeprops=WedgeProperties,\n",
    "                                    textprops=dict(color = \"black\")) #values that change the pie\n",
    "    Ax.legend(wedges,LegendPreview,title = \"Legend\",loc= 'right', \n",
    "              bbox_to_anchor = (1,0,0.5,1)) #Parameters for the legend\n",
    "    plt.setp(autotexts,size = 7,weight = \"bold\") #parameters for the texts inside the pie\n",
    "    plt.title(LegendTitle,y=1.08) #Legend Location\n",
    "    plt.show() #show the pie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the text in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "    DataFrame = LoadFile(\"eclass_all_with_sentiment_v2.pkl\")#Load the Dataframe\n",
    "    DataFrame = DataFrame.astype(str).apply(lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))\n",
    "    DataFrame['text'] = DataFrame['text'].str.lower() #make all lowercase\n",
    "    stop  = stopwords.words('english') \n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "    DataFrame['text'] = DataFrame['text'].str.replace(pat, '',regex = True)\n",
    "    DataFrame['text'] = DataFrame['text'].str.replace(r'\\s+', ' ',regex = True)\n",
    "    cleaned = []\n",
    "    elements = list(DataFrame['text'])\n",
    "    for i in elements:\n",
    "        FlteredText = re.sub('https?://[A-Za-z0-9./]+','',i)\n",
    "        FlteredText = re.sub(\"[^a-zA-Z0-9]\", \" \",FlteredText)\n",
    "        cleaned.append(re.sub(r'^RT[\\s]+', '', FlteredText))\n",
    "    DataFrame['text'] = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Data analysis 1\n",
    "    Neg = DataFrame.groupby('sentiment').get_group(\"NEG\") #Create dataframes for each of the sentiment group\n",
    "    Pos = DataFrame.groupby('sentiment').get_group(\"POS\")\n",
    "    Neu = DataFrame.groupby('sentiment').get_group(\"NEU\")\n",
    "    DataStr = []\n",
    "    DataValues = []\n",
    "    DataStr.append(\"NEG\") #append the strs NEG POS NEU so they appear in the legend\n",
    "    DataStr.append(\"POS\")\n",
    "    DataStr.append(\"NEU\")\n",
    "    DataValues.append(len(Neg)) #we append the length of each dataframe so we can find the percentage that each sentiment hold in the dataframe\n",
    "    DataValues.append(len(Pos))\n",
    "    DataValues.append(len(Neu))\n",
    "    ExplodeData = (0.1, 0.0, 0.2)\n",
    "    Colors = ( \"brown\", \"cyan\", \"orange\") #some nice colors\n",
    "    Preview = \"Distribution of sentiments in the tweets\" #a bad title\n",
    "    CreatePlot(ExplodeData,Colors,DataValues,DataStr,DataStr,Preview) #call the create plot to handle the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Data analysis 2\n",
    "    MostCommon = Counter(\" \".join(DataFrame[\"text\"]).split()).most_common(10) #take the most common words in the dataframe  along with how many times they appear\n",
    "    MostCommonStr = [] #the above call returns a list of tuples\n",
    "    MostCommonValues = []\n",
    "    MostCommonStr,MostCommonValues = SplitTuple(MostCommon) #split the tuple into 2 lists\n",
    "    ExplodeData = (0.1, 0.15, 0.2, 0.3, 0.05, 0.25, 0.3,0.4, 0.4 ,0.0)\n",
    "    Colors = ( \"orange\", \"cyan\", \"brown\",\n",
    "          \"grey\", \"indigo\", \"beige\",\"yellow\",\"red\",\"pink\",\"blue\") #some nice colors again\n",
    "    Preview = \"Most common words in the DataFrame\" #a meh title\n",
    "    CreatePlot(ExplodeData,Colors,MostCommonValues,MostCommonStr,MostCommonStr,Preview)#let create plot handle the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Data analysis 3\n",
    "    MostCommonStr = []\n",
    "    MostCommonValues = [] \n",
    "    Neg = DataFrame.groupby('sentiment').get_group(\"NEG\") #group by sentiment\n",
    "    Pos = DataFrame.groupby('sentiment').get_group(\"POS\")\n",
    "    Neu = DataFrame.groupby('sentiment').get_group(\"NEU\")\n",
    "    MostCommonNeg = Counter(\" \".join(Neg[\"text\"]).split()).most_common(3) #take the 3 most common words by each sentiment\n",
    "    MostCommonPos = Counter(\" \".join(Pos[\"text\"]).split()).most_common(3) #this method returns a list of tuples\n",
    "    MostCommonNeu = Counter(\" \".join(Neu[\"text\"]).split()).most_common(3) #within each there are the words and the times they appear in the dataframe\n",
    "    MostCommonNegStr,MostCommonNegValues = SplitTuple(MostCommonNeg) #split the given tuples\n",
    "    MostCommonPosStr,MostCommonPosValues = SplitTuple(MostCommonPos)\n",
    "    MostCommonNeuStr,MostCommonNeuValues = SplitTuple(MostCommonNeu)\n",
    "    for items in MostCommonNegStr: #append the most common words in tweets with negative sentiments into a list\n",
    "        MostCommonStr.append(items) \n",
    "    for items in MostCommonNegValues: #append the time each word appears in the dataframe\n",
    "        MostCommonValues.append(items)\n",
    "    for items in MostCommonPosStr: #append the most common words in tweets with positive sentiments into a list\n",
    "        MostCommonStr.append(items)\n",
    "    for items in MostCommonPosValues: #append the time each word appears in the dataframe\n",
    "        MostCommonValues.append(items)\n",
    "    for items in MostCommonNeuStr: #append the most common words in tweets with neutral sentiments into a list\n",
    "        MostCommonStr.append(items)\n",
    "    for items in MostCommonNeuValues: #append the time each word appears in the dataframe\n",
    "        MostCommonValues.append(items)\n",
    "    SentimentArray = [\"NEG is red\",\"POS is cyan\",\"NEU is orange\"]\n",
    "    ExplodeData = (0.1,0.1,0.1, 0.0,0.0,0.0, 0.2,0.2,0.2)\n",
    "    Colors = ( \"red\",\"red\",\"red\",\"cyan\",\"cyan\",\"cyan\",\"orange\",\"orange\",\"orange\",) #some cool colors\n",
    "    Preview = \"Most common words in the DataFrame by sentiment\" #a nice title\n",
    "    CreatePlot(ExplodeData,Colors,MostCommonValues,MostCommonStr,SentimentArray,Preview) #create plot handles the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Data analysis 4\n",
    "    SentimentValues = []\n",
    "    SentimentStr = []\n",
    "    Astra = DataFrame.loc[DataFrame[\"text\"].str.contains(\"astrazeneca\",case=True)] #find all the tweets containing the words astrazeneca\n",
    "    Pfizer = DataFrame.loc[DataFrame[\"text\"].str.contains(pat = \"moderna|pfizer|biontech\",case=True)]  #find all the tweets containing the words moderna pfizer or biontech\n",
    "    AstraNeg = Astra.groupby('sentiment').get_group(\"NEG\") #group each group by sentiment\n",
    "    AstraPos = Astra.groupby('sentiment').get_group(\"POS\")\n",
    "    AstraNeu = Astra.groupby('sentiment').get_group(\"NEU\")\n",
    "    PfizerNeg = Pfizer.groupby('sentiment').get_group(\"NEG\")\n",
    "    PfizerPos = Pfizer.groupby('sentiment').get_group(\"POS\")\n",
    "    PfizerNeu = Pfizer.groupby('sentiment').get_group(\"NEU\")\n",
    "    SentimentValues.append(len(AstraNeg)) #append to the values list the length of each group\n",
    "    SentimentValues.append(len(PfizerNeg))\n",
    "    SentimentValues.append(len(AstraNeu))\n",
    "    SentimentValues.append(len(PfizerNeu))\n",
    "    SentimentValues.append(len(AstraPos))\n",
    "    SentimentValues.append(len(PfizerPos))\n",
    "    SentimentStr.append(\"Astra NEG\") #append the sentiment and the group name in the list\n",
    "    SentimentStr.append(\"Pfizer NEG\")\n",
    "    SentimentStr.append(\"Astra NEU\")\n",
    "    SentimentStr.append(\"Pfizer NEU\")\n",
    "    SentimentStr.append(\"Astra POS\")\n",
    "    SentimentStr.append(\"Pfizer POS\")\n",
    "    Colors = ('orange','yellow','orange','yellow','orange','yellow') #nice colors\n",
    "    ExplodeData = (0.1,0.1,0.1,0.2,0.2,0.2) \n",
    "    SentimentArray = [\"Astra is orange\",\"Pfizer etc. is yellow\"]\n",
    "    Preview = \"Sentiment Comparison between Astrazeneca tweets and Pfizer etc\"#a really bad title\n",
    "    CreatePlot(ExplodeData,Colors,SentimentValues,SentimentStr,SentimentArray,Preview) #let create plot handle the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Data analysis 5\n",
    "    DataFrame['month'] = pd.DatetimeIndex(DataFrame['date']).month #Create a column in the dataframe containing the month the tweet was sent\n",
    "    ExplodeData = (0.1, 0.0, 0.2, 0.3, 0.0, 0.0, 0.1,0.2, 0.2 ,0.0,0.2,0.3)\n",
    "    Colors = ( \"orange\", \"cyan\", \"brown\",\n",
    "          \"grey\", \"indigo\", \"beige\",\"purple\",\"red\",\"pink\",\"blue\",\"yellow\",\"green\") #more cool colors\n",
    "    Preview = \"Tweets per month\" #a nice title\n",
    "    Date = [dict() for i in range(12)]\n",
    "    DateStr = [\"Jan\",\"Feb\",\"March\",\"April\",\"May\",\"June\",\"July\",\"Aug\",\"Sept\",\"Oct\",\"Nov\",\"Dec\"] #all the months in a list\n",
    "    DateValues = []\n",
    "    for i in range(1,13):\n",
    "        Date[i - 1] = DataFrame.groupby('month').get_group(i)#get for each month all the tweets sent\n",
    "        DateValues.append(len(Date[i - 1])) #get the length of the column of the above dictionary\n",
    "    CreatePlot(ExplodeData,Colors,DateValues,DateStr,DateStr,Preview) #create the plot\n",
    "    AverageTweets = int(len(DataFrame)/12) #get the average tweets sent\n",
    "    ImportantMonths = []\n",
    "    for i in range(1,13):\n",
    "        if DateValues[i - 1] > AverageTweets:\n",
    "            ImportantMonths.append(i) #find the important months(those who are above the average tweets)\n",
    "    #the rest of the data analysis 5 was not implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train, Test = sklearn.model_selection.train_test_split(DataFrame,test_size=0.2,random_state=42, shuffle=True) #split the dataframe(for some reason the DataFrame variable is unrecognized but the code runs correctly)\n",
    "train_path = \"Train.tsv\"\n",
    "test_path = \"Test.tsv\"\n",
    "Train.to_csv(train_path,sep='\\t') #save as tsv by using the tab as a separator parameter\n",
    "Test.to_csv(test_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization of the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "Train = OpenFile(\"Train.tsv\") #open  the tsv files\n",
    "Test = OpenFile(\"Test.tsv\")\n",
    "vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=1000,stop_words='english') #create a count vectorizer\n",
    "TrainVector = vectorizer.fit_transform(Train['text']) #transform the texts in train and test dataframes\n",
    "TestVector = vectorizer.transform(Test['text'])\n",
    "dfTrain_bow_sklearn = pd.DataFrame(TrainVector.toarray(),columns=vectorizer.get_feature_names_out()) #create dataframes by the numpy arrays created\n",
    "dfTest_bow_sklearn = pd.DataFrame(TestVector.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "SaveFile(dfTrain_bow_sklearn,'bagofwordsTrain.pkl') #save as pickle files in order to save time\n",
    "SaveFile(dfTest_bow_sklearn,'bagofwordsTest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "Train = OpenFile(\"Train.tsv\") #of the tsv files\n",
    "Test = OpenFile(\"Test.tsv\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000,stop_words='english') #set parameterers for the vectorizer\n",
    "tfidfTrain_vector = tfidf_vectorizer.fit_transform(Train['text']) #transform the text columns\n",
    "tfidfTest_vector = tfidf_vectorizer.transform(Test['text'])\n",
    "tfidfTrain_df = pd.DataFrame(tfidfTrain_vector.toarray(),columns=tfidf_vectorizer.get_feature_names_out()) #create a dataframe from that transformation\n",
    "tfidfTest_df = pd.DataFrame(tfidfTest_vector.toarray(),columns=tfidf_vectorizer.get_feature_names_out())\n",
    "SaveFile(tfidfTrain_df,'TF-IDFTrain.pkl') #save the dataframes to pickle files so they can easily be accessed\n",
    "SaveFile(tfidfTest_df,'TF-IDFTest.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
